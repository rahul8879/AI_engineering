{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple LangChain Legacy Chain Example\n",
    "\n",
    "This notebook shows how to create a simple legacy LLM Chain using LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Reference Article ( Please follow the same order as given):\n",
    "1. https://python.langchain.com/docs/introduction/\n",
    "2. https://python.langchain.com/docs/concepts/architecture/\n",
    "3. https://python.langchain.com/docs/integrations/providers/\n",
    "4. https://python.langchain.com/docs/concepts/lcel/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all required library\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI #https://pypi.org/project/langchain-openai/0.2.12/\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#load the .env : \n",
    "load_dotenv(r'C:\\Users\\Rahul\\Documents\\FREELANCING\\1.AppliedSkil\\Gen_AI_learning\\.env')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "# create a prompt template to accept user queries\n",
    "prompt_txt = \"{query}\"\n",
    "prompt_template = ChatPromptTemplate.from_template(prompt_txt)\n",
    "chain = prompt_template | chatgpt | StrOutputParser() #https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html\n",
    "response =  chain.invoke({'query' : 'Explain Generative AI in 1 line'}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring LLMs and ChatModels for LLM Input / Output with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model I/O\n",
    "\n",
    "In LangChain, the central part of any application is the language model. This module provides crucial tools for working effectively with any language model, ensuring it integrates smoothly and communicates well.\n",
    "\n",
    "### Key Components of Model I/O\n",
    "\n",
    "**LLMs and Chat Models (used interchangeably):**\n",
    "- **LLMs:**\n",
    "  - **Definition:** Pure text completion models.\n",
    "  - **Input/Output:** Receives a text string and returns a text string.\n",
    "- **Chat Models:**\n",
    "  - **Definition:** Based on a language model but with different input and output types.\n",
    "  - **Input/Output:** Takes a list of chat messages as input and produces a chat message as output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Models and LLMs\n",
    "\n",
    "Large Language Models (LLMs) are a core component of LangChain. LangChain does not implement or build its own LLMs. It provides a standard API for interacting with almost every LLM out there.\n",
    "\n",
    "There are lots of LLM providers (OpenAI, Hugging Face, etc) - the LLM class is designed to provide a standard interface for all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Accessing ChatGPT as an LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied skills wo skills hain jo kisi specific field ya profession mein practical use ke liye develop ki jati hain. Ye skills theoretical knowledge ko real-world situations mein apply karne mein madad karti hain. Applied skills sikhne se aapko kuch important cheezein sikhne ko milti hain:\n",
      "\n",
      "1. **Problem-Solving**: Aapko real-life problems ko samajhne aur unka hal nikalne ki ability milti hai.\n",
      "\n",
      "2. **Critical Thinking**: Aapko sochne ki, analyze karne ki aur decisions lene ki skills develop hoti hain.\n",
      "\n",
      "3. **Communication**: Aapko apne ideas aur thoughts ko effectively communicate karne ki ability milti hai, chahe wo verbal ho ya written.\n",
      "\n",
      "4. **Teamwork**: Aapko dusre logon ke saath kaam karne aur collaborative efforts mein contribute karne ki skills milti hain.\n",
      "\n",
      "5. **Technical Skills**: Specific tools, software, ya techniques ka istemal karne ki ability develop hoti hai jo aapke profession ke liye zaroori hoti hain.\n",
      "\n",
      "6. **Time Management**: Aapko apne tasks ko prioritize karne aur time ko effectively manage karne ki skills milti hain.\n",
      "\n",
      "7. **Adaptability**: Aapko naye situations aur challenges ke saath adapt hone ki ability milti hai.\n",
      "\n",
      "8. **Leadership**: Aapko leadership qualities develop karne ka mauka milta hai, jo aapko team ko lead karne mein madad karti hain.\n",
      "\n",
      "In skills ka istemal aapke career mein growth aur success ke liye bahut zaroori hota hai. Applied skills sikhne se aapko practical experience milta hai jo aapko job market mein competitive banata hai.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "prompt_txt = \"{query}\"\n",
    "prompt_template = ChatPromptTemplate.from_template(prompt_txt)\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "response = chatgpt.invoke('Applied skills kya sikhati hai')\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing ChatGPT as an Chat Model LLM\n",
    " ### Refer first code block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Open LLMs with HuggingFace Serverless API\n",
    "\n",
    "The free [serverless API](https://huggingface.co/inference-api/serverless) lets you implement solutions and iterate in no time, but it may be rate limited for heavy use cases, since the loads are shared with other requests.\n",
    "\n",
    "For enterprise workloads, you can use Inference Endpoints - Dedicated which would be hosted on a specific cloud instance of your choice and would have a cost associated with it. Here we will use the free serverless API which works quite well in most cases.\n",
    "\n",
    "The advantage is you do not need to download the models or run them locally on a GPU compute infrastructure which takes time and also would cost you a fair amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing Microsoft Phi-3 Mini Instruct\n",
    "\n",
    "The Phi-3-Mini-4K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties. Check more details [here](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! wait_for_model is not default parameter.\n",
      "                    wait_for_model was transferred to model_kwargs.\n",
      "                    Please make sure that wait_for_model is what you intended.\n",
      "c:\\Users\\Rahul\\Documents\\FREELANCING\\1.AppliedSkil\\Gen_AI_learning\\ai-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# API_URL doesn't work now, so we are using repo_id of the model\n",
    "repo_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "phi3_params = {\n",
    "                  \"wait_for_model\": True, # waits if model is not available in Hugginface serve\n",
    "                  \"do_sample\": False, # greedy decoding - temperature = 0\n",
    "                  \"return_full_text\": False, # don't return input prompt\n",
    "                  \"max_new_tokens\": 1000, # max tokens answer can go upto\n",
    "                }\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    # max_length=128,\n",
    "    temperature=0.5,\n",
    "    # huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
    "   **phi3_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. बकचोड़ी एक प्रकार का स्थानीय मध्य भारतीय संस्थान है, जिसमें उन लोगों की जाने के लिए उपलब्ध उद्यम हैं।\n",
      "2. बकचोड़ी में विभिन्न कार्यक्रमों का उपलब्धि है, जैसे कि स्थानीय मार्गों को बनाने, गावों की सुरक्षा और सामाजिक संबंधों को बनाने के लिए।\n",
      "3. बकचोड़ी के स्थानीय संस्थानों में स्थानीय समाज की समर्थन और समर्थन के लिए जाने का काम करते हैं, जिसमें स्थानीय लोगों को समर्थन और समर्थन के लिए मदद करते हैं।\n"
     ]
    }
   ],
   "source": [
    "# Phi3 expects input prompt to be formatted in a specific way\n",
    "# check more details here: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\n",
    "phi3_prompt = \"\"\"<|user|>Explain what is bakchodi in 3 bullet points in hindi<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "response = llm.invoke(phi3_prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! wait_for_model is not default parameter.\n",
      "                    wait_for_model was transferred to model_kwargs.\n",
      "                    Please make sure that wait_for_model is what you intended.\n"
     ]
    }
   ],
   "source": [
    "gemma_repo_id = \"google/gemma-1.1-2b-it\"\n",
    "\n",
    "gemma_params = {\n",
    "                  \"wait_for_model\": True, # waits if model is not available in Hugginface serve\n",
    "                  \"do_sample\": False, # greedy decoding - temperature = 0\n",
    "                  \"return_full_text\": False, # don't return input prompt\n",
    "                  \"max_new_tokens\": 1000, # max tokens answer can go upto\n",
    "                }\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=gemma_repo_id,\n",
    "    **gemma_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Explain what is bakchodi in 3 bullet points in hindi\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "1. प्रारोभिक शिक्षा का एक प्रकार\n",
      "2. विविधता का एक उपकम\n",
      "3. शिक्षा का मूलभूत अवस्था।\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
